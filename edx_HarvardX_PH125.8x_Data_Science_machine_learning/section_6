library(dslabs)
mnist <- read_mnist()

names(mnist)
dim(mnist$train$images)

class(mnist$train$labels)
table(mnist$train$labels)

# sample 10k rows from training set, 1k rows from test set
set.seed(1990)
index <- sample(nrow(mnist$train$images), 10000)
x <- mnist$train$images[index,]
y <- factor(mnist$train$labels[index])

index <- sample(nrow(mnist$test$images), 1000)
x_test <- mnist$test$images[index,]
y_test <- factor(mnist$test$labels[index])

library(matrixStats)
library(ggplot2)
sds <- colSds(x)
qplot(sds, bins = 256, color = I("black"))

library(caret)
nzv <- nearZeroVar(x)
image(matrix(1:784 %in% nzv, 28, 28))

col_index <- setdiff(1:ncol(x), nzv)
length(col_index)


# kNN
colnames(x) <- 1:ncol(mnist$train$images)
colnames(x_test) <- colnames(x)

control <- trainControl(method = "cv", number = 10, p = .9)
train_knn <- train(x[,col_index], y,
                   method = "knn", 
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = control)

n <- 1000
b <- 2
index <- sample(nrow(x), n)
control <- trainControl(method = "cv", number = b, p = .9)
train_knn <- train(x[index ,col_index], y[index],
                   method = "knn",
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = control)
fit_knn <- knn3(x[ ,col_index], y,  k = 3)

y_hat_knn <- predict(fit_knn,
                     x_test[, col_index],
                     type="class")
cm <- confusionMatrix(y_hat_knn, factor(y_test))
cm$overall["Accuracy"]

cm$byClass[,1:2]

# random forest
library(randomForest)
control <- trainControl(method="cv", number = 5)
grid <- data.frame(mtry = c(1, 5, 10, 25, 50, 100))
train_rf <-  train(x[, col_index], y,
                   method = "rf",
                   nTree = 150,
                   trControl = control,
                   tuneGrid = grid,
                   nSamp = 5000)

fit_rf <- randomForest(x[, col_index], y,
                       minNode = train_rf$bestTune$mtry)

y_hat_rf <- predict(fit_rf, x_test[ ,col_index])
cm <- confusionMatrix(y_hat_rf, y_test)
cm$overall["Accuracy"]





# 
models <- c("glm", "lda", "naive_bayes", "knn", "gamLoess", "qda", "rf")

library(caret)
library(dslabs)
library(tidyverse)
set.seed(1)
data("mnist_27")

fits <- lapply(models, function(model){ 
  print(model)
  train(y ~ ., method = model, data = mnist_27$train)
}) 

names(fits) <- models

# Q2
length(mnist_27$test$y)
length(fits)

temp <- sapply(fits, function(x){predict(x, mnist_27$test)})
dim(temp)

# Q3
#cm <- apply(temp, 2, function(x){confusionMatrix(factor(x, levels=levels(mnist_27$test$y)), mnist_27$test$y)})
cm <- colMeans(temp == mnist_27$test$y)
cm
mean(cm)

# Q4
temp
ensemble <- apply(temp, 1, function(x){
  if (mean(x == 7) > 0.5) 7 else 2
})
ensemble

mean(ensemble == mnist_27$test$y)

# Q5
sum(cm > mean(ensemble == mnist_27$test$y))
which(cm > mean(ensemble == mnist_27$test$y))

# Q6
mean(sapply(fits, function(x) min(x$results$Accuracy)))

# Q7
idx <- which(sapply(fits, function(x) min(x$results$Accuracy)) >= 0.8)

ensemble <- apply(temp[,idx], 1, function(x){
  if (mean(x == 7) > 0.5) 7 else 2
})
ensemble
mean(ensemble == mnist_27$test$y)


# movie recommendation
library(caret)
set.seed(755)
test_index <- createDataPartition(y = movielens$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- movielens[-test_index,]
test_set <- movielens[test_index,]

test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

mu_hat <- mean(train_set$rating)
mu_hat

naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse


mu <- mean(train_set$rating) 

movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
model_1_rmse

train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")

user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)

model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
model_2_rmse


#
library(tidyverse)
library(lubridate)
library(dslabs)
data("movielens")

# Q1

head(movielens)

movie_rating_counts <- movielens %>%
  group_by(movieId) %>%
  summarise(n=n())

movie_data <- movie_rating_counts %>%
  left_join(movielens, by = "movieId") %>%
  filter(!is.na(year))

ggplot(movie_data, aes(x = as.factor(year), y = n)) +
  geom_boxplot() +
  #scale_y_sqrt() +
  labs(
    x = "Year of Release",
    y = "Number of Ratings (sqrt scale)",
    title = "Distribution of Ratings per Movie by Release Year"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

q1 <- movie_data %>%
  group_by(year) %>%
  summarise(md=median(n)) %>%
  arrange(desc(md))

temp <- movie_data %>%
  group_by(movieId) %>%
  filter(year >= 1993) %>%
  mutate(n_year=n/(2018 - first(year))) %>%
  arrange(desc(n_year)) %>%
  mutate(avg_rating=mean(rating)) %>%
  distinct(movieId, title, avg_rating, n_year)

ggplot(temp, aes(x=n_year, y=avg_rating)) +
  geom_point() +
  geom_smooth()

# Q5
movielens <- mutate(movielens, date = as_datetime(timestamp))

# Q6
movielens %>% mutate(date = as_datetime(timestamp)) %>%
  mutate(week = round_date(date, unit="week")) %>%
  group_by(week) %>%
  summarise(avg_rating=mean(rating)) %>%
  ggplot(aes(week, avg_rating)) +
  geom_point() + geom_smooth()

# Q8
genre <- movielens %>% 
  group_by(genres) %>%
  summarize(
    n=n(),
    avg_rating=mean(rating),
    se=sd(rating)/sqrt(n),
    .groups = "drop"
  ) %>%
  filter(n>1000)

ggplot(genre, aes(x = fct_reorder(genres, avg_rating), y = avg_rating)) +
  geom_errorbar(aes(ymin = avg_rating - se, ymax = avg_rating + se), width = 0.2) +
  coord_flip() +
  labs(
    title = "Average Movie Rating by Genre Combination",
    x = "Genre Combination",
    y = "Average Rating Â± SE"
  ) +
  theme_minimal()


# 6.3 regularization
library(tidyverse)
options(digits=7)

set.seed(1986)
n <- round(2^rnorm(1000, 8, 1))
set.seed(1)
mu <- round(80 + 2 * rt(1000, 5))
range(mu)

schools <- data.frame(id = paste("PS",1:100), 
                      size = n, 
                      quality = mu,
                      rank = rank(-mu))

schools %>% top_n(10, quality) %>% arrange(desc(quality))

scores <- sapply(1:nrow(schools), function(i){
  scores <- rnorm(n=schools$size[i], mean=schools$quality[i], sd=30)
  scores
})

schools <- schools %>% mutate(score = sapply(scores, mean))

#Q1
head(schools)
schools %>% top_n(10, score) %>% arrange(desc(score)) %>% select(id, size, score)

#Q2
median(schools$size)
schools %>% top_n(10, score) %>% summarize(md=median(size))
schools %>% top_n(-10, score) %>% summarize(md=median(size))

#Q3
ggplot(schools, aes(x=size, y=score)) +
  geom_point() +
  geom_smooth()

#Q5
overall <- mean(sapply(scores, mean))
alpha <- 25
score_reg <- sapply(scores, function(x) overall + sum(x-overall)/(length(x)+alpha))

schools %>% mutate(score_reg=score_reg) %>%
  top_n(n=10, score_reg) %>%
  arrange(desc(score_reg))

#Q6
alpha <- seq(10,250)
temp <- sapply(alpha, function(y) {
  score_reg <- sapply(scores, function(x) {overall + sum(x-overall)/(length(x)+y)
    })
  sqrt(mean((score_reg - schools$quality)^2))
  })
alpha[which.min(temp)]

#Q7
alpha <- 135
score_reg <- sapply(scores, function(x) overall + sum(x-overall)/(length(x)+alpha))

schools %>% mutate(score_reg=score_reg) %>%
  top_n(n=10, score_reg) %>%
  arrange(desc(score_reg))

#Q8
alpha <- seq(10,250)
temp <- sapply(alpha, function(y) {
  score_reg <- sapply(scores, function(x) {sum(x)/(length(x)+y)
  })
  sqrt(mean((score_reg - schools$quality)^2))
})
alpha[which.min(temp)]


#
library(tidyverse)
library(lubridate)
library(dslabs)
data("movielens")

train_small <- movielens %>% 
  group_by(movieId) %>%
  filter(n() >= 50 | movieId == 3252) %>% ungroup() %>%
  group_by(userId) %>%
  filter(n() >= 50) %>% ungroup()

y <- train_small %>% 
  dplyr::select(userId, movieId, rating) %>%
  pivot_wider(names_from = "movieId", values_from = "rating") %>%
  as.matrix()

rownames(y)<- y[,1]
y <- y[,-1]

movie_titles <- movielens %>% 
  dplyr::select(movieId, title) %>%
  distinct()

class(colnames(y))
as.integer(colnames(y))[1:10]
class(movie_titles$movieId)
colnames(y) <- with(movie_titles, title[match(as.integer(colnames(y)), movieId)])
colnames(y)[1:10]

class(movie_titles$movieId)
class(colnames(y))

y <- sweep(y, 1, rowMeans(y, na.rm=TRUE))
y <- sweep(y, 2, colMeans(y, na.rm=TRUE))

#Q1
set.seed(1987)
n <- 100
k <- 8
Sigma <- 64  * matrix(c(1, .75, .5, .75, 1, .5, .5, .5, 1), 3, 3) 
m <- MASS::mvrnorm(n, rep(0, 3), Sigma)
m <- m[order(rowMeans(m), decreasing = TRUE),]
y <- m %x% matrix(rep(1, k), nrow = 1) + matrix(rnorm(matrix(n*k*3)), n, k*3)
colnames(y) <- c(paste(rep("Math",k), 1:k, sep="_"),
                 paste(rep("Science",k), 1:k, sep="_"),
                 paste(rep("Arts",k), 1:k, sep="_"))

my_image <- function(x, zlim = range(x), ...){
  colors = rev(RColorBrewer::brewer.pal(9, "RdBu"))
  cols <- 1:ncol(x)
  rows <- 1:nrow(x)
  image(cols, rows, t(x[rev(rows),,drop=FALSE]), xaxt = "n", yaxt = "n",
        xlab="", ylab="",  col = colors, zlim = zlim, ...)
  abline(h=rows + 0.5, v = cols + 0.5)
  axis(side = 1, cols, colnames(x), las = 2)
}

my_image(y)

my_image(cor(y), zlim = c(-1,1))
range(cor(y))
axis(side = 2, 1:ncol(y), rev(colnames(y)), las = 2)

s <- svd(y)
names(s)

y_svd <- s$u %*% diag(s$d) %*% t(s$v)
max(abs(y - y_svd))

#Q3
head(y)
ss_y <- colSums(y*y)
sum(ss_y)

ss_yv <- colSums(y_svd*y_svd)
sum(ss_yv) 

#Q4
plot(ss_y)
plot(ss_yv)

#Q5
data.frame(x = sqrt(ss_yv), y = s$d) %>%
  ggplot(aes(x,y)) +
  geom_point()

#Q6
#Q8
plot(rowMeans(y), -s$u[,1]*s$d[1])

#Q9
my_image(s$v)


