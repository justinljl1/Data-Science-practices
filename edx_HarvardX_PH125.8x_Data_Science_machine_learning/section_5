### Section 5: The Caret Package
## Section 5: The Caret Package
library(tidyverse)
library(dslabs)
data("mnist_27")

library(caret)
train_glm <- train(y ~ ., method = "glm", data = mnist_27$train)
train_knn <- train(y ~ ., method = "knn", data = mnist_27$train)

y_hat_glm <- predict(train_glm, mnist_27$test, type = "raw")
y_hat_knn <- predict(train_knn, mnist_27$test, type = "raw")

confusionMatrix(y_hat_glm, mnist_27$test$y)$overall[["Accuracy"]]
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall[["Accuracy"]]

getModelInfo("knn")
modelLookup("knn")

ggplot(train_knn, highlight = TRUE)

data.frame(k = seq(9, 67, 2))

set.seed(2008)
train_knn <- train(y ~ ., method = "knn",
                   data = mnist_27$train,
                   tuneGrid = data.frame(k = seq(9, 71, 2)))

ggplot(train_knn, highlight = TRUE)

train_knn$bestTune
train_knn$finalModel

confusionMatrix(predict(train_knn, mnist_27$test, type = "raw"),
                mnist_27$test$y)$overall["Accuracy"]

control <- trainControl(method = "cv", number = 10, p = .9)
train_knn_cv <- train(y ~ ., method = "knn",
                      data = mnist_27$train,
                      tuneGrid = data.frame(k = seq(9, 71, 2)),
                      trControl = control)
ggplot(train_knn_cv, highlight = TRUE)

names(train_knn_cv$results)

plot_cond_prob <- function(p_hat=NULL){
  tmp <- mnist_27$true_p
  if(!is.null(p_hat)){
    tmp <- mutate(tmp, p=p_hat)
  }
  tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
    geom_raster(show.legend = FALSE) +
    scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
    stat_contour(breaks=c(0.5),color="black")
}

plot_cond_prob(predict(train_knn, mnist_27$true_p, type = "prob")[,2])


#
install.packages("gam")
modelLookup("gamLoess")

grid <- expand.grid(span = seq(0.15, 0.65, len = 10), degree = 1)

train_loess <- train(y ~ ., 
                     method = "gamLoess",
                     tuneGrid=grid,
                     data = mnist_27$train)
ggplot(train_loess, highlight = TRUE)

confusionMatrix(data = predict(train_loess, mnist_27$test), 
                reference = mnist_27$test$y)$overall["Accuracy"]

p1 <- plot_cond_prob(predict(train_loess, mnist_27$true_p, type = "prob")[,2])
p1

# Q1
library(tidyverse)
library(caret)

set.seed(1996)
n <- 1000
p <- 10000
x <- matrix(rnorm(n*p), n, p)
colnames(x) <- paste("x", 1:ncol(x), sep = "_")
y <- rbinom(n, 1, 0.5) %>% factor()

x_subset <- x[ ,sample(p, 100)]
set.seed(1)
fit <- train(x_subset, y, method = "glm")
fit$results

# Q2
pvals <- rep(0, ncol(x))
for (i in 1:ncol(x)) {
  pvals[i] <- t.test(x[,i][y==0], x[,i][y==1], var.equal=TRUE)$p.value
}

ind <- which(pvals < 0.01)
length(ind)

# Q3
x_subset <- x[ ,ind]
set.seed(1)
fit <- train(x_subset, y, method = "glm")
fit$results

# Q4
set.seed(1)
fit <- train(x_subset, y, method = "knn", tuneGrid = data.frame(k = seq(101, 301, 25)))
ggplot(fit)

# Q5

## Decision tree
# Load tidyverse
library(tidyverse)

# load package for decision tree
library(rpart)

# load the dslabs package
library(dslabs)

# fit a classification tree using the polls_2008 dataset, 
# which contains only one predictor (day)
# and the outcome (margin)
fit <- rpart(margin ~ ., data = polls_2008)

# display the decision tree
plot(fit, margin = 0.1)
text(fit, cex = 0.75)

# examine the fit from the classification tree model
polls_2008 %>%  
  mutate(y_hat = predict(fit)) %>% 
  ggplot() +
  geom_point(aes(day, margin)) +
  geom_step(aes(day, y_hat), col="red")

# fit a classification tree on the mnist data using cross validation
train_rpart <- train(y ~ .,
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 25)),
                     data = mnist_27$train)
# and plot it
plot(train_rpart)

# compute accuracy
confusionMatrix(predict(train_rpart, mnist_27$test), mnist_27$test$y)$overall["Accuracy"]

# view the final decision tree
plot(train_rpart$finalModel, margin = 0.1) # plot tree structure
text(train_rpart$finalModel) # add text labels

# load library for random forest
library(randomForest)
train_rf <- randomForest(y ~ ., data=mnist_27$train)
confusionMatrix(predict(train_rf, mnist_27$test), mnist_27$test$y)$overall["Accuracy"]

# use cross validation to choose parameter
train_rf_2 <- train(y ~ .,
                    method = "Rborist",
                    tuneGrid = data.frame(predFixed = 2, minNode = c(3, 50)),
                    data = mnist_27$train)
confusionMatrix(predict(train_rf_2, mnist_27$test), mnist_27$test$y)$overall["Accuracy"]

# Q1
library(rpart)
n <- 1000
sigma <- 0.25
set.seed(1)
x <- rnorm(n, 0, 1)
y <- 0.75 * x + rnorm(n, 0, sigma)
dat <- data.frame(x = x, y = y)

fit <- rpart(y ~ ., data = dat)

# Q2
plot(fit, margin = 0.1)
text(fit, cex = 0.75)

# Q3
dat %>% 
  mutate(y_hat = predict(fit)) %>% 
  ggplot() +
  geom_point(aes(x, y)) +
  geom_smooth(aes(y_hat, x), col=2)

# Q4
library(randomForest)
fit <- randomForest(y ~ x, data = dat) 
dat %>% 
  mutate(y_hat = predict(fit)) %>% 
  ggplot() +
  geom_point(aes(x, y)) +
  geom_step(aes(x, y_hat), col = "red")

# Q5
plot(fit, margin = 0.1)

# Q6
fit <- randomForest(y ~ x, data = dat, nodesize = 25, maxnodes = 25)
dat %>% 
  mutate(y_hat = predict(fit)) %>% 
  ggplot() +
  geom_point(aes(x, y)) +
  geom_step(aes(x, y_hat), col = "red")

##
# Q1
library(rpart)
library(caret)

data("tissue_gene_expression")
set.seed(1991)
train_rpart <- train(tissue_gene_expression$x, tissue_gene_expression$y, method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.1,0.01)))
plot(train_rpart)
which.max(train_rpart$results$Accuracy)

# Q2
set.seed(1991)
train_rpart <- train(tissue_gene_expression$x, tissue_gene_expression$y, method = "rpart",
                     control = rpart.control(minsplit=0), 
                     tuneGrid = data.frame(cp = seq(0, 0.1,0.01)))
plot(train_rpart)
train_rpart$results
which.max(train_rpart$results$Accuracy)

# Q3
plot(train_rpart$finalModel)
text(train_rpart$finalModel, cex=0.75)

# Q4
set.seed(1991)
train_rf <- train(tissue_gene_expression$x, tissue_gene_expression$y, method = "rf",
                  nodesize=1,
                  tuneGrid = data.frame(mtry = seq(50, 200, 25)))

train_rf$results
ggplot(train_rf)


## 5.2: Titanic Exercises
library(titanic)    # loads titanic_train data frame
library(caret)
library(tidyverse)
library(rpart)

# 3 significant digits
options(digits = 3)

# clean the data - `titanic_train` is loaded with the titanic package
titanic_clean <- titanic_train %>%
  mutate(Survived = factor(Survived),
         Embarked = factor(Embarked),
         Age = ifelse(is.na(Age), median(Age, na.rm = TRUE), Age), # NA age to median age
         FamilySize = SibSp + Parch + 1) %>%    # count family members
  select(Survived,  Sex, Pclass, Age, Fare, SibSp, Parch, FamilySize, Embarked)

# Q1
set.seed(42)
test_index <- createDataPartition(titanic_clean$Survived, times=1, p=0.2, list=FALSE)
test_set <- titanic_clean[test_index,]
train_set <- titanic_clean[-test_index,]
dim(train_set)
dim(test_set)
prop.table(table(test_set$Survived))

# Q2
set.seed(3)
guess <- sample(c(0,1), nrow(test_set), replace=T)
mean(guess == test_set$Survived)

# Q3a
head(train_set)
prop.table(table(train_set[train_set$Sex == "female",]$Survived))
prop.table(table(train_set[train_set$Sex == "male",]$Survived))

# Q3b
y_s <- ifelse(test_set$Sex == "male", 0, 1) %>% 
  factor(levels = levels(test_set$Survived))

mean(y_s == test_set$Survived)

# Q4a
train_set %>% group_by(Pclass) %>% summarise(suv = mean(Survived == 1))

# Q4b
y_class <- ifelse(test_set$Pclass == 1, 1, 0) %>% 
  factor(levels = levels(test_set$Survived))

mean(y_class == test_set$Survived)

# Q4c
train_set %>% group_by(Pclass, Sex) %>% summarise(suv = mean(Survived == 1))

#Q4d
y_cb <- ifelse(test_set$Pclass <3 & test_set$Sex == "female", 1, 0) %>% 
  factor(levels = levels(test_set$Survived))
mean(y_cb == test_set$Survived)

# Q5a
lapply(list(y_s, y_class, y_cb), function(x) confusionMatrix(x, test_set$Survived))

# Q5b

# Q6
lapply(list(y_s, y_class, y_cb), function(x) F_meas(x, test_set$Survived))

# Q7
set.seed(1)
fit_loess <- train(Survived ~ Fare, method = "gamLoess", data = train_set)
confusionMatrix(predict(fit_loess, test_set), test_set$Survived)

# Q8
set.seed(1)
fit_loess <- train(Survived ~ Age, method = "glm", data = train_set)
confusionMatrix(predict(fit_loess, test_set), test_set$Survived)

set.seed(1)
fit_loess <- train(Survived ~ Age + Sex + Pclass + Fare, method = "glm", data = train_set)
confusionMatrix(predict(fit_loess, test_set), test_set$Survived)

fit_loess <- train(Survived ~ ., method = "glm", data = train_set)
confusionMatrix(predict(fit_loess, test_set), test_set$Survived)

# Q9
set.seed(6)
train_knn <- train(Survived ~ ., method = "knn", data = train_set,
                  tuneGrid = data.frame(k = seq(3,51,2)))
train_knn$bestTune

plot(train_knn)

confusionMatrix(predict(train_knn, test_set), test_set$Survived)

# Q10
set.seed(8)
control <- trainControl(method="cv", number=10)
train_knn <- train(Survived ~ ., method = "knn", data = train_set,
                   trControl = control,
                   tuneGrid = data.frame(k = seq(3,51,2)))
train_knn$bestTune
mean(predict(train_knn, test_set) == test_set$Survived)

# Q11a
set.seed(10)
train_rpt <- train(Survived ~ ., method = "rpart", data = train_set,
                   tuneGrid = data.frame(cp = seq(0,0.05,0.002)))
train_rpt$bestTune
mean(predict(train_rpt, test_set) == test_set$Survived)

# Q11b
plot(train_rpt$finalModel)
text(train_rpt$finalModel, cex=0.75)

# Q12
set.seed(14)
train_rf <- train(Survived ~ ., method = "rf", data = train_set,
                   ntree = 100, 
                   tuneGrid = data.frame(mtry = seq(1:7)))
train_rf$bestTune
mean(predict(train_rf, test_set) == test_set$Survived)
