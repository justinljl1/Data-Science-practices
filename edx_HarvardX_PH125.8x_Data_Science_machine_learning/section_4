### Section 4: Cross-validation and k-Nearest Neighbors
#
library(tidyverse)
library(caret)
library(dslabs)
library(gridExtra)
library(tidyverse)

data("mnist_27")

mnist_27$test %>%
  ggplot(aes(x_1, x_2, color = y)) +
  geom_point()

knn_fit <- knn3(y ~ ., data = mnist_27$train)

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]

fit_lm <- mnist_27$train %>% 
  mutate(y = ifelse(y == 7, 1, 0)) %>% 
  lm(y ~ x_1 + x_2, data = .)
p_hat_lm <- predict(fit_lm, mnist_27$test)
y_hat_lm <- factor(ifelse(p_hat_lm > 0.5, 7, 2))
confusionMatrix(y_hat_lm, mnist_27$test$y)$overall["Accuracy"]

plot_cond_prob <- function(p_hat=NULL){
  tmp <- mnist_27$true_p
  if(!is.null(p_hat)){
    tmp <- mutate(tmp, p=p_hat)
  }
  tmp %>% ggplot(aes(x_1, x_2, z=p, fill=p)) +
    geom_raster(show.legend = FALSE) +
    scale_fill_gradientn(colors=c("#F8766D", "white", "#00BFC4")) +
    stat_contour(breaks=c(0.5), color="black")
}

p1 <- plot_cond_prob() +
  ggtitle("True conditional probability")
p2 <- plot_cond_prob(predict(knn_fit, mnist_27$true_p)[,2]) +
  ggtitle("kNN-5 estimate")
grid.arrange(p2, p1, nrow=1)

y_hat_knn <- predict(knn_fit, mnist_27$train, type = "class")
confusionMatrix(y_hat_knn, mnist_27$train$y)$overall["Accuracy"]

y_hat_knn <- predict(knn_fit, mnist_27$test, type = "class")
confusionMatrix(y_hat_knn, mnist_27$test$y)$overall["Accuracy"]

# Q1
library(dslabs)
data("heights")

set.seed(1)
test_index <- createDataPartition(heights$sex, times=1, p=0.5, list=FALSE)
test <- heights[test_index,]
train <- heights[-test_index,]

k <- seq(1,101,3)
F1_score <- sapply(k, function(x){
  knn_fit <- knn3(sex ~ height, k=x, data = train)
  y_hat_knn <- predict(knn_fit, test, type = "class") %>%
    factor(levels = levels(train$sex))
  F_meas(y_hat_knn, test$sex)
})
max(F1_score)
which.max(F1_score)

# Q2
library(dslabs)
library(caret)
library(class)
data("tissue_gene_expression")

x <- tissue_gene_expression$x
y <- tissue_gene_expression$y

set.seed(1)
test_index <- createDataPartition(y, times=1, p=0.5, list=FALSE)
x_train <- x[-test_index, , drop = FALSE]
y_train <- y[-test_index]
x_test <- x[test_index, , drop = FALSE]
y_test <- y[test_index]

k = seq(1, 11, 2)
mean_score <- sapply(k, function(k){
  y_hat_knn <- knn(train = x_train, test = x_test, cl = y_train, k=k)
  mean(y_hat_knn == y_test)
})

mean_score

#set.seed(1)
#library(caret)
#y <- tissue_gene_expression$y
#x <- tissue_gene_expression$x
#train_index <- createDataPartition(y, list = FALSE)
#sapply(seq(1, 11, 2), function(k){
#  fit <- knn3(x[train_index,], y[train_index], k = k)
#  y_hat <- predict(fit, newdata = data.frame(x=x[-train_index,]),
#                   type = "class")
#  mean(y_hat == y[-train_index])
#})


#
library(dslabs)
library(caret)

data(mnist_27)

set.seed(1995)
indexes <- createResample(mnist_27$train$y, 10)
table(indexes$Resample01)

x=sapply(indexes, function(x){
  sum(x == 3)
})
x
sum(x)


# Q3
set.seed(1)
B <- 10^4
M <- replicate(B, {
  X <- rnorm(100, 0, 1)
  quantile(X, 0.75)
})

mean(M)
sd(M)

# Q4
set.seed(1)
y <- rnorm(100, 0, 1)

set.seed(1)
B <- 10^4
M_star <- replicate(B, {
  X_star <- sample(y, 100, replace = TRUE)
  quantile(X_star, 0.75)
})

mean(M_star)
sd(M_star)
