### 2.1 Multiple Regression
# one observation at a time
# multiple (vector x vector) -> data matrix x vector

## exervise 2.1.1
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from helper import fit_and_plot_linear, fit_and_plot_multi
%matplotlib inline

f = pd.read_csv('Advertising.csv')
df.head()

df_results = pd.DataFrame(columns=['Predictor', 'R2 Train', 'R2 Test'])

fit_and_plot_linear(df[['TV']])
fit_and_plot_linear(df[['Radio']])
fit_and_plot_linear(df[['Newspaper']])
fit_and_plot_multi()

for col in ['TV', 'Radio', 'Newspaper']:
    r2_train, r2_test = fit_and_plot_linear(df[[col]])
    df_results.loc[len(df_results)] = [col, r2_train, r2_test]

r2_train, r2_test = fit_and_plot_multi()
df_results.loc[len(df_results)] = ['multi', r2_train, r2_test]

df_results.head()

## exervise 2.1.2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import preprocessing
from prettytable import PrettyTable
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
%matplotlib inline

df = pd.read_csv('Advistising.csv')
head(df)

mse_list = list()
df.head()

mse_list = list()
predictors = df.columns[:-1]
cols = []
n = len(predictors)
for r in range(1, n + 1):
    def combine(start=0, current=[]):
        if len(current) == r:
            cols.append(current[:])
            return
        for i in range(start, n):
            current.append(predictors[i])
            combine(i + 1, current)
            current.pop()
    combine()

print(cols)

for i in cols:
    x = df[i]
    y = df['Sales']
    x_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.8,random_state=0)
    lreg = LinearRegression()
    fit = lreg.fit(x_train, y_train)
    y_pred = lreg.predict(x_test)
    MSE = mean_squared_error(y_test, y_pred)
    mse_list.append(MSE)

t = PrettyTable(['Predictors', 'MSE'])
for i in range(len(mse_list)):
    t.add_row([cols[i],round(mse_list[i],3)])
print(t)


### 2.2 Techniques for Multilinear Modeling
# categoric variable -> dummy variable (2 values) or nested(?) dummy variables (2+ values)
# overall accuracy, type 1 error, type 2 error

## exercise 2.2.1
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

df = pd.read_csv('credit.csv')
df.head()

x = df.drop('Balance', axis=1)
y = df['Balance']

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)

try:
    test_model = LinearRegression().fit(x_train, y_train)
except Exception as e:
    print('Error!:', e)

df.dtypes

# Fit a linear model using only the numeric features in the dataframe.
numeric_features = x.select_dtypes(include='number').columns
print(numeric_features)
model1 = LinearRegression().fit(x_train[numeric_features], y_train)

train_score = model1.score(x_train[numeric_features], y_train)
test_score = model1.score(x_test[numeric_features], y_test)
print('Train R2:', train_score)
print('Test R2:', test_score)

# Create x train and test design matrices creating dummy variables for the categorical while keeping the numeric feature columns unchanged.
# hint: use pd.get_dummies() with the drop_first hyperparameter for this
x_train_design = pd.get_dummies(x_train, drop_first=True).astype(int)
x_test_design = pd.get_dummies(x_test, drop_first=True).astype(int)
x_train_design.head()

# Fit model2 on design matrix
model2 = LinearRegression().fit(x_train_design, y_train)

train_score = model2.score(x_train_design, y_train)
test_score = model2.score(x_test_design, y_test)
print('Train R2:', train_score)
print('Test R2:', test_score)

# Fit a model to predict Balance from 2 predictors: Income and the best categorical feature
features = ['Income', 'Student_Yes']
model3 = LinearRegression()
model3.fit(x_train_design[features], y_train)

beta0 = model3.intercept_
beta1 = model3.coef_[features.index('Income')]
beta2 = model3.coef_[features.index(best_cat_feature)]

x_space = np.linspace(x['Income'].min(), x['Income'].max(), 1000)

# Generate 2 sets of predictions based on best categorical feature value.
# When categorical feature is true/present (1)
y_hat_yes = beta0 + beta1 * x_space + beta2 * 1
# When categorical feature is false/absent (0)
y_hat_no = beta0 + beta1 * x_space + beta2 * 0

# Plot the 2 prediction lines for students and non-students.
ax = sns.scatterplot(data=pd.concat([x_train_design, y_train], axis=1), x='Income', y='Balance', hue=best_cat_feature, alpha=0.8)
ax.plot(x_space, y_hat_no)
ax.plot(x_space, y_hat_yes);

## exercise 2.2.2
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

df = pd.read_csv('Advertising.csv', index_col=0)
df.head()
X = df.drop('Sales', axis=1)
y = df.Sales.values
lm = LinearRegression().fit(X,y)

df *= 1000
df.head()
X = df.drop('Sales', axis=1)
y = df.Sales.values
lm = LinearRegression().fit(X,y)

plt.figure(figsize=(8,3))
cols = X.columns
coefs = lm.coef_
plt.barh(cols, coefs)
plt.axvline(0, c='k', ls='--', alpha=0.5)
plt.ylabel('Predictor')
plt.xlabel('Coefficient Values')
plt.title('Coefficients of Linear Model Predicting Sales\n from Newspaper, '\
            'Radio, and TV Advertising Budgets (in Dollars)');

X2 = pd.DataFrame()
X2['TV (Rupee)'] = 200 * df['TV'] # convert to Sri Lankan Rupee
X2['Radio (Won)'] = 1175 * df['Radio'] # convert to South Korean Won
X2['Newspaper (Cedi)'] = 6 * df['Newspaper'] # Convert to Ghanaian Cedi
lm2 = LinearRegression().fit(X2,y)

# nomenclature
# normalize (linear scaling into 0-1 range) + standardize (Gaussian scaling around origin) vs. normalize (gaussian scaling around origin) + standardize (linear scaling into 0-1 range)


## exercise 2.2.3
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pprint import pprint
from sklearn.linear_model import LinearRegression
$matplotlin inline

df = pd.read_csv('colinearity.csv')
df.head()

X = df.drop(['y'],axis=1)
y = df['y'].values

linear_coef = []
for i in X:
    x = df[[i]]
    linreg = LinearRegression()
    linreg.fit(x,y)
    linear_coef.append(linreg.coef_)
print(coef_df)

multi_linear = LinearRegression()
multi_linear.fit(X,y)
multi_coef = multi_linear.coef_[0]
print(multi_coef)

corrMatrix = df[['x1','x2','x3','x4']].corr() 
sns.heatmap(corrMatrix, annot=True) 
plt.show()


### 2.3 Polynomial Regression
#
