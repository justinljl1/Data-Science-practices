### 2.1 Multiple Regression
# one observation at a time
# multiple (vector x vector) -> data matrix x vector

## exervise 2.1.1
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from helper import fit_and_plot_linear, fit_and_plot_multi
%matplotlib inline

f = pd.read_csv('Advertising.csv')
df.head()

df_results = pd.DataFrame(columns=['Predictor', 'R2 Train', 'R2 Test'])

fit_and_plot_linear(df[['TV']])
fit_and_plot_linear(df[['Radio']])
fit_and_plot_linear(df[['Newspaper']])
fit_and_plot_multi()

for col in ['TV', 'Radio', 'Newspaper']:
    r2_train, r2_test = fit_and_plot_linear(df[[col]])
    df_results.loc[len(df_results)] = [col, r2_train, r2_test]

r2_train, r2_test = fit_and_plot_multi()
df_results.loc[len(df_results)] = ['multi', r2_train, r2_test]

df_results.head()

## exervise 2.1.2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import preprocessing
from prettytable import PrettyTable
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
%matplotlib inline

df = pd.read_csv('Advistising.csv')
head(df)

mse_list = list()
df.head()

mse_list = list()
predictors = df.columns[:-1]
cols = []
n = len(predictors)
for r in range(1, n + 1):
    def combine(start=0, current=[]):
        if len(current) == r:
            cols.append(current[:])
            return
        for i in range(start, n):
            current.append(predictors[i])
            combine(i + 1, current)
            current.pop()
    combine()

print(cols)

for i in cols:
    x = df[i]
    y = df['Sales']
    x_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.8,random_state=0)
    lreg = LinearRegression()
    fit = lreg.fit(x_train, y_train)
    y_pred = lreg.predict(x_test)
    MSE = mean_squared_error(y_test, y_pred)
    mse_list.append(MSE)

t = PrettyTable(['Predictors', 'MSE'])
for i in range(len(mse_list)):
    t.add_row([cols[i],round(mse_list[i],3)])
print(t)


### 2.2 Techniques for Multilinear Modeling
# categoric variable -> dummy variable (2 values) or nested(?) dummy variables (2+ values)
# overall accuracy, type 1 error, type 2 error

## exercise 2.2.1
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

df = pd.read_csv('credit.csv')
df.head()

x = df.drop('Balance', axis=1)
y = df['Balance']

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)

try:
    test_model = LinearRegression().fit(x_train, y_train)
except Exception as e:
    print('Error!:', e)

df.dtypes

# Fit a linear model using only the numeric features in the dataframe.
numeric_features = x.select_dtypes(include='number').columns
print(numeric_features)
model1 = LinearRegression().fit(x_train[numeric_features], y_train)

train_score = model1.score(x_train[numeric_features], y_train)
test_score = model1.score(x_test[numeric_features], y_test)
print('Train R2:', train_score)
print('Test R2:', test_score)

# Create x train and test design matrices creating dummy variables for the categorical while keeping the numeric feature columns unchanged.
# hint: use pd.get_dummies() with the drop_first hyperparameter for this
x_train_design = pd.get_dummies(x_train, drop_first=True).astype(int)
x_test_design = pd.get_dummies(x_test, drop_first=True).astype(int)
x_train_design.head()

# Fit model2 on design matrix
model2 = LinearRegression().fit(x_train_design, y_train)

train_score = model2.score(x_train_design, y_train)
test_score = model2.score(x_test_design, y_test)
print('Train R2:', train_score)
print('Test R2:', test_score)

# Fit a model to predict Balance from 2 predictors: Income and the best categorical feature
features = ['Income', 'Student_Yes']
model3 = LinearRegression()
model3.fit(x_train_design[features], y_train)

beta0 = model3.intercept_
beta1 = model3.coef_[features.index('Income')]
beta2 = model3.coef_[features.index(best_cat_feature)]

x_space = np.linspace(x['Income'].min(), x['Income'].max(), 1000)

# Generate 2 sets of predictions based on best categorical feature value.
# When categorical feature is true/present (1)
y_hat_yes = beta0 + beta1 * x_space + beta2 * 1
# When categorical feature is false/absent (0)
y_hat_no = beta0 + beta1 * x_space + beta2 * 0

# Plot the 2 prediction lines for students and non-students.
ax = sns.scatterplot(data=pd.concat([x_train_design, y_train], axis=1), x='Income', y='Balance', hue=best_cat_feature, alpha=0.8)
ax.plot(x_space, y_hat_no)
ax.plot(x_space, y_hat_yes);

## exercise 2.2.2
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

df = pd.read_csv('Advertising.csv', index_col=0)
df.head()
X = df.drop('Sales', axis=1)
y = df.Sales.values
lm = LinearRegression().fit(X,y)

df *= 1000
df.head()
X = df.drop('Sales', axis=1)
y = df.Sales.values
lm = LinearRegression().fit(X,y)

plt.figure(figsize=(8,3))
cols = X.columns
coefs = lm.coef_
plt.barh(cols, coefs)
plt.axvline(0, c='k', ls='--', alpha=0.5)
plt.ylabel('Predictor')
plt.xlabel('Coefficient Values')
plt.title('Coefficients of Linear Model Predicting Sales\n from Newspaper, '\
            'Radio, and TV Advertising Budgets (in Dollars)');

X2 = pd.DataFrame()
X2['TV (Rupee)'] = 200 * df['TV'] # convert to Sri Lankan Rupee
X2['Radio (Won)'] = 1175 * df['Radio'] # convert to South Korean Won
X2['Newspaper (Cedi)'] = 6 * df['Newspaper'] # Convert to Ghanaian Cedi
lm2 = LinearRegression().fit(X2,y)

# nomenclature
# normalize (linear scaling into 0-1 range) + standardize (Gaussian scaling around origin) vs. normalize (gaussian scaling around origin) + standardize (linear scaling into 0-1 range)


## exercise 2.2.3
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pprint import pprint
from sklearn.linear_model import LinearRegression
$matplotlin inline

df = pd.read_csv('colinearity.csv')
df.head()

X = df.drop(['y'],axis=1)
y = df['y'].values

linear_coef = []
for i in X:
    x = df[[i]]
    linreg = LinearRegression()
    linreg.fit(x,y)
    linear_coef.append(linreg.coef_)
print(coef_df)

multi_linear = LinearRegression()
multi_linear.fit(X,y)
multi_coef = multi_linear.coef_[0]
print(multi_coef)

corrMatrix = df[['x1','x2','x3','x4']].corr() 
sns.heatmap(corrMatrix, annot=True) 
plt.show()


### 2.3 Polynomial Regression
## exercise 2.3.1
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from helper import get_poly_pred
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
%matplotlib inline

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

def get_poly_pred(x_train, x_test, y_train, degree=1):
    # Generate polynomial features on the train data
    x_poly_train= PolynomialFeatures(degree=degree).fit_transform(x_train)
    # Generate polynomial features on the test data
    print(x_train.shape, x_test.shape, y_train.shape)
    x_poly_test= PolynomialFeatures(degree=degree).fit_transform(x_test)
    # Initialize a model to perform polynomial regression
    polymodel = LinearRegression()
    # Fit the model on the polynomial transformed train data
    polymodel.fit(x_poly_train, y_train)
    # Predict on the entire polynomial transformed test data
    y_poly_pred = polymodel.predict(x_poly_test)
    return y_poly_pred

df = pd.read_csv('poly.csv')
df.head()

x = df[['x']].values
y = df['y'].values

fig, ax = plt.subplots()
ax.plot(x,y,'x')
ax.set_xlabel('$x$ values')
ax.set_ylabel('$y$ values')
ax.set_title('$y$ vs $x$')
plt.show();

# linear model
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state = 22 )
model = LinearRegression()
model.fit(x_train, y_train)
y_lin_pred = model.predict(x_test)

fig, ax = plt.subplots()
ax.plot(x,y,'x', label='data')
ax.set_xlabel('$x$ values')
ax.set_ylabel('$y$ values')
ax.plot(x_test, y_lin_pred, label='linear model predictions')
plt.legend();

# polynominal model
guess_degree = 5
y_poly_pred = get_poly_pred(x_train, x_test, y_train, degree=guess_degree) 

idx = np.argsort(x_test[:,0])
x_test = x_test[idx]
y_test = y_test[idx]

y_lin_pred = y_lin_pred[idx]
y_poly_pred= y_poly_pred[idx]

plt.scatter(x, y, s=10, label="Test Data")

plt.plot(x_test,y_lin_pred,label="Linear fit", color='k')
plt.plot(x_test, y_poly_pred, label="Polynomial fit",color='red', alpha=0.6)
plt.xlabel("x values")
plt.ylabel("y values")
plt.legend()
plt.show()

poly_residuals = y_test - y_poly_pred
lin_residuals = y_test - y_lin_pred

fig, ax = plt.subplots(1,2, figsize = (10,4))
bins = np.linspace(-20,20,20)
ax[0].set_xlabel('Residuals')
ax[0].set_ylabel('Frequency')

ax[0].hist(poly_residuals, bins, label = 'poly residuals', color='#B2D7D0', alpha=0.6)
ax[0].hist(lin_residuals, bins, label = 'linear residuals', color='#EFAEA4', alpha=0.6)
ax[0].legend(loc = 'upper left')
ax[1].hlines(0,-75,75, color='k', ls='--', alpha=0.3, label='Zero residual')
ax[1].scatter(y_poly_pred, poly_residuals, s=10, color='#B2D7D0', label='Polynomial predictions')
ax[1].scatter(y_lin_pred, lin_residuals, s = 10, color='#EFAEA4', label='Linear predictions' )
ax[1].set_xlim(-75,75)
ax[1].set_xlabel('Predicted values')
ax[1].set_ylabel('Residuals')
ax[1].legend(loc = 'upper left')
fig.suptitle('Residual Analysis (Linear vs Polynomial)')
plt.show();

## exercise 2.3.2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
%matplotlib inline

df = pd.read_csv('poly.csv')
x = df[['x']].values
y = df['y'].values
df.head()

fig, ax = plt.subplots()
ax.plot(x,y,'x')
ax.set_xlabel('$x$ values')
ax.set_ylabel('$y$ values')
ax.set_title('$y$ vs $x$');

model = LinearRegression()
model.fit(x,y)
y_lin_pred = model.predict(x)

guess_degree = 5
x_poly= PolynomialFeatures(degree=guess_degree).fit_transform(x)
polymodel = LinearRegression(fit_intercept=False)
polymodel.fit(x_poly,y)
y_poly_pred = polymodel.predict(x_poly)

x_l = np.linspace(np.min(x),np.max(x),100).reshape(-1, 1)
y_lin_pred_l = model.predict(x_l)

x_poly_l= PolynomialFeatures(degree=guess_degree).fit_transform(x_l)
y_poly_pred_l = polymodel.predict(x_poly_l)

plt.scatter(x, y, s=10, label="Data")
plt.plot(x_l,y_lin_pred_l,label="Linear fit")
plt.plot(x_poly_l, y_poly_pred_l, label="Polynomial fit")
plt.xlabel("x values")
plt.ylabel("y values")
plt.legend()
plt.show()
